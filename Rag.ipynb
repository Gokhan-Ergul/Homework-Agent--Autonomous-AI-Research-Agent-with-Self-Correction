{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9e56ecc-27f0-4150-afd7-8c13111ca1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b94e510-91ae-423b-bbd5-ed268057e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block[0]: The x0 coordinate (the left side of the block).\n",
    "#\n",
    "#block[1]: The y0 coordinate (the top side of the block).\n",
    "#\n",
    "#block[2]: The x1 coordinate (the right side of the block).\n",
    "#\n",
    "#block[3]: The y1 coordinate (the bottom side of the block).\n",
    "#\n",
    "#block[4]: The actual string of text contained in the block.\n",
    "#\n",
    "#block[5]: The sequential number of the block on the page.\n",
    "#\n",
    "#block[6]: The type of the block.\n",
    "\n",
    "#0 = a text block\n",
    "\n",
    "#1 = an image block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1de45735-26b7-49de-9064-9094ed2b0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_blocks(pdf_path):\n",
    "    all_blocks = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            if doc.name == \"A Survey on the Memory Mechanism of Large Language Model based Agents.pdf\" or doc.name == \"The Rise and Potential of Large Language Model Based Agents.pdf\":\n",
    "                doc.delete_pages(from_page = 1, to_page = 2)\n",
    "            \n",
    "            for page in doc:\n",
    "                blocks = page.get_text('blocks')\n",
    "                for block in blocks:\n",
    "                    if block[6] == 0:\n",
    "                        block_text = block[4].strip()\n",
    "                        all_blocks.append(block_text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    return all_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "817e0a6a-34df-414d-b2d4-753206470cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_references(text_blocks: list[str]) -> list[str]:\n",
    "    \n",
    "    REFERENCE_HEADING_REGEX = re.compile(r\"^(references|bibliography|works cited)$\", re.IGNORECASE)\n",
    "    \n",
    "    APPENDIX_HEADING_REGEX = re.compile(r\"^(Appendix|[A-Z])(\\.|\\s+)\")\n",
    "    \n",
    "    clean_blocks = []\n",
    "    in_references_section = False\n",
    "\n",
    "    for block_text in text_blocks:\n",
    "        block_text_clean = block_text.strip()\n",
    "        block_text_lower = block_text_clean.lower()\n",
    "\n",
    "        if in_references_section:\n",
    "            if APPENDIX_HEADING_REGEX.match(block_text_clean):\n",
    "                in_references_section = True \n",
    "                continue\n",
    "            else:\n",
    "                continue \n",
    "        \n",
    "        elif REFERENCE_HEADING_REGEX.match(block_text_lower):\n",
    "            in_references_section = True \n",
    "            continue\n",
    "\n",
    "        if not in_references_section:\n",
    "            clean_blocks.append(block_text)\n",
    "            \n",
    "    return clean_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a00ce21-fdaf-4ebf-a69e-4bb4ca60b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_noise_and_captions(text_blocks: list[str]) -> list[str]:\n",
    "\n",
    "    caption_regex = re.compile(r\"^(Figure|Fig\\.|Table)\\s+\\d+[:\\.]?\", re.IGNORECASE)\n",
    "    \n",
    "    header_regex = re.compile(\n",
    "        r\"^(Published in|Front\\. Comput\\. Sci\\.|arXiv:|https|^\\d+$)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    \n",
    "    noise_regex = re.compile(r\"^(\\*|†|‡)\")\n",
    "    \n",
    "    heading_regex = re.compile(r\"^\\d+(\\.\\d+)*\\s+[A-Za-z]\")\n",
    "    \n",
    "    figure_content_regex = re.compile(\n",
    "        r\"^(<LM>|<<run:|<<<read:|tennis_balls =|calculate\\.py|answer = tennis_balls)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    MIN_LENGTH = 25 \n",
    "\n",
    "    final_clean_blocks = []\n",
    "    \n",
    "    for block_text in text_blocks:\n",
    "        \n",
    "        if (header_regex.match(block_text) or \n",
    "            noise_regex.match(block_text) or \n",
    "            figure_content_regex.match(block_text)):\n",
    "            continue\n",
    "\n",
    "        if caption_regex.match(block_text):\n",
    "            continue \n",
    "\n",
    "        if heading_regex.match(block_text):\n",
    "            final_clean_blocks.append(block_text)\n",
    "            continue\n",
    "            \n",
    "        if len(block_text) < MIN_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        final_clean_blocks.append(block_text)\n",
    "            \n",
    "    return final_clean_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e267e29-01fd-4c58-b3ea-1d8149e66ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total block number: 254\n",
      "Main blocks: 119\n",
      "Last block number: 104.\n",
      "==================================================\n",
      "total block number: 518\n",
      "Main blocks: 402\n",
      "Last block number: 249.\n",
      "==================================================\n",
      "total block number: 600\n",
      "Main blocks: 413\n",
      "Last block number: 309.\n",
      "==================================================\n",
      "total block number: 443\n",
      "Main blocks: 229\n",
      "Last block number: 155.\n",
      "==================================================\n",
      "total block number: 1314\n",
      "Main blocks: 600\n",
      "Last block number: 428.\n",
      "==================================================\n",
      "total block number: 205\n",
      "Main blocks: 170\n",
      "Last block number: 87.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "all_document_data = []\n",
    "pdf_file1 = \"A Review of Prominent Paradigms for LLMBased_Agent_Tool_Use_Including_RAG _Planning_and_Feedback_Learning.pdf\"\n",
    "\n",
    "pdf_file2 = \"A Survey on Large Language Model based Autonomous Agents.pdf\"\n",
    "pdf_file3 = \"A Survey on the Memory Mechanism of Large Language Model based Agents.pdf\"\n",
    "pdf_file4 = \"Augmented Language Models.pdf\"\n",
    "pdf_file5 = \"The Rise and Potential of Large Language Model Based Agents.pdf\"\n",
    "pdf_file6 = \"Understanding the planning of LLM agents.pdf\"\n",
    "\n",
    "#doc.save(pdf_file3_output,garbage = 4, deflate = True)\n",
    "\n",
    "\n",
    "pdf_list =[pdf_file1, pdf_file2,pdf_file3, pdf_file4,pdf_file5, pdf_file6]\n",
    "\n",
    "for pdf in pdf_list:\n",
    "    \n",
    "    all_blocks = get_all_blocks(pdf)\n",
    "    print(f\"total block number: {len(all_blocks)}\")\n",
    "    \n",
    "    main_blocks = filter_references(all_blocks)\n",
    "    print(f\"Main blocks: {len(main_blocks)}\")\n",
    "    \n",
    "    final_clean_blocks = filter_noise_and_captions(main_blocks)\n",
    "    print(f\"Last block number: {len(final_clean_blocks)}.\")\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(final_clean_blocks)\n",
    "    \n",
    "    document_info1 = {\n",
    "        \"text_content\": final_text,\n",
    "        \"source_file\": pdf\n",
    "    }\n",
    "    all_document_data.append(document_info1)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704632e3-180d-4144-aac2-23b66a20e37f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c71f3947-37dd-4571-8f3e-881e22c6cc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text_content', 'source_file'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_document_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ed926e4-e874-4205-a42b-bf651ccbf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunk = []\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "for doc in all_document_data:\n",
    "    text_content = doc['text_content']\n",
    "    source_file = doc['source_file']\n",
    "\n",
    "    chunks = text_splitter.create_documents([text_content])\n",
    "\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        chunk.metadata['source'] = source_file\n",
    "        chunk.metadata['chunk_index'] = i\n",
    "\n",
    "    all_chunk.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce296da6-3a01-4371-a98e-b2d69096266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = 'all_chunk_data.pkl'\n",
    "with open(file_path,\"wb\") as file:\n",
    "    pickle.dump(all_chunk,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f6f221a-d0ff-40ac-8563-56bd01ae9233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n",
      "(Including RAG), Planning, and Feedback Learning\n",
      "\n",
      "Xinzhe Li *\n",
      "Independent Researcher\n",
      "sergioli212@outlook.com\n"
     ]
    }
   ],
   "source": [
    "print(all_chunk[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c17d9bc-6541-48c0-b34d-097bd5728108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'A Review of Prominent Paradigms for LLMBased_Agent_Tool_Use_Including_RAG _Planning_and_Feedback_Learning.pdf', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_chunk[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64964de4-803a-4140-8b3a-9f63d5cd3e32",
   "metadata": {},
   "source": [
    "# \"R\" Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8728cff-d5df-4f22-999e-d3f5f6b841bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "877d41c7-ada9-480b-a49e-a9eaefea05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "349ac93b-1a22-4d6b-b5f4-de73e95c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1effb8d-464d-4de8-b7e6-e6a1f5b73a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_directory = \"rag_db\"\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=all_chunk,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=db_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6af2b0a8-531e-4adc-8060-01cf9f33ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "How to Evaluate the Memory in LLM-based Agent\n",
      "\n",
      "How to effectively evaluate the memory module remains an open problem, where diverse evaluation\n",
      "strategies have been proposed in previous works according to different applications. To clearly\n",
      "show the common ideas of different evaluation methods, in this section, we summarize a general\n",
      "framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\n",
      "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\n",
      "indirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\n",
      "be effectively accomplished, the memory module is demonstrated to be useful.\n",
      "\n",
      "6.1\n",
      "Direct Evaluation\n",
      "\n",
      "---\n",
      "\n",
      "6\n",
      "How to Evaluate the Memory in LLM-based Agent\n",
      "\n",
      "How to effectively evaluate the memory module remains an open problem, where diverse evaluation\n",
      "strategies have been proposed in previous works according to different applications. To clearly\n",
      "show the common ideas of different evaluation methods, in this section, we summarize a general\n",
      "framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\n",
      "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\n",
      "indirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\n",
      "be effectively accomplished, the memory module is demonstrated to be useful.\n",
      "\n",
      "6.1\n",
      "Direct Evaluation\n",
      "\n",
      "---\n",
      "\n",
      "6\n",
      "How to Evaluate the Memory in LLM-based Agent\n",
      "\n",
      "How to effectively evaluate the memory module remains an open problem, where diverse evaluation\n",
      "strategies have been proposed in previous works according to different applications. To clearly\n",
      "show the common ideas of different evaluation methods, in this section, we summarize a general\n",
      "framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\n",
      "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\n",
      "indirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\n",
      "be effectively accomplished, the memory module is demonstrated to be useful.\n",
      "\n",
      "6.1\n",
      "Direct Evaluation\n",
      "\n",
      "---\n",
      "\n",
      "6\n",
      "How to Evaluate the Memory in LLM-based Agent\n",
      "\n",
      "How to effectively evaluate the memory module remains an open problem, where diverse evaluation\n",
      "strategies have been proposed in previous works according to different applications. To clearly\n",
      "show the common ideas of different evaluation methods, in this section, we summarize a general\n",
      "framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\n",
      "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\n",
      "indirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\n",
      "be effectively accomplished, the memory module is demonstrated to be useful.\n",
      "\n",
      "6.1\n",
      "Direct Evaluation\n",
      "\n",
      "---\n",
      "\n",
      "6\n",
      "How to Evaluate the Memory in LLM-based Agent\n",
      "\n",
      "How to effectively evaluate the memory module remains an open problem, where diverse evaluation\n",
      "strategies have been proposed in previous works according to different applications. To clearly\n",
      "show the common ideas of different evaluation methods, in this section, we summarize a general\n",
      "framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\n",
      "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\n",
      "indirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\n",
      "be effectively accomplished, the memory module is demonstrated to be useful.\n",
      "\n",
      "6.1\n",
      "Direct Evaluation\n"
     ]
    }
   ],
   "source": [
    "query = \"How to effectively evaluate the memory module?\"\n",
    "results_with_scores = vector_db.similarity_search_with_score(query, k=5)\n",
    "score_threshold = 0.8\n",
    "relevant_docs = [doc for doc, score in results_with_scores if score < score_threshold]\n",
    "\n",
    "if not relevant_docs:\n",
    "    print(\"No sufficiently similar answer was found for this question.\")\n",
    "else:\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    print(context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5a59c-3e89-4af7-ae61-a3309df54d24",
   "metadata": {},
   "source": [
    "# \"A\" Augmented and \"G\" Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd9cd032-8a02-4d56-8bdd-0511ef5840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever,MultiQueryRetriever\n",
    "\n",
    "prompt_template_string = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Act as a helpful expert. Provide a clear and direct answer to the question using only the information in the context.\n",
    "- You can perform simple calculations like unit conversions (e.g., pounds to kg) to make the answer more helpful.\n",
    "- If the answer is not in the context, state that the document does not contain this information.\n",
    "- Answer directly without starting your response with \"Based on the context....\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template = prompt_template_string,\n",
    "    input_variables = ['context', 'question']\n",
    ")\n",
    "\n",
    "bm25_retriver = BM25Retriever.from_documents(\n",
    "    documents=all_chunk\n",
    ")\n",
    "\n",
    "similarity_retriever = vector_db.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {'k':7}\n",
    ")\n",
    "\n",
    "ensemble_retriver = EnsembleRetriever(\n",
    "    retrievers = [bm25_retriver,similarity_retriever],\n",
    "    weights = [0.3,0.7]\n",
    ")\n",
    "multiquery_esemble_retriever = MultiQueryRetriever.from_llm(\n",
    "    llm = llm, \n",
    "    retriever = ensemble_retriver\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    retriever = multiquery_esemble_retriever,\n",
    "    chain_type_kwargs = {\"prompt\": custom_prompt},\n",
    "    return_source_documents = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a613d85e-70cb-41f5-95f4-56d21f1d276f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How to effectively evaluate the memory module?',\n",
       " 'result': \"To effectively evaluate the memory module, two broad strategies can be employed: direct evaluation and indirect evaluation.\\n\\n**1. Direct Evaluation:**\\nThis strategy independently measures the capability of the memory module.\\n*   **Process:**\\n    *   **Human Evaluator Selection:** Evaluators should be familiar with the evaluation task and have diverse backgrounds to minimize subjective biases.\\n    *   **Output Labeling:** This can be done by directly scoring the results to obtain absolute and quantitative evaluations, or by making comparisons between two candidates to reduce labeling noises.\\n    *   **Rating Granularity:** The granularity of ratings should be carefully designed; not too coarse (which may not effectively discriminate capabilities) nor too fine-grained (which may require excessive effort for judgments).\\n*   **Metrics:**\\n    *   **F1-score:** Calculated as `2 * Precision * Recall`, where `Precision = TP / (TP + FP)` (True Positives / (True Positives + False Positives)) and `Recall = TP / (TP + FN)` (True Positives / (True Positives + False Negatives)). This is used to evaluate retrieval.\\n    *   **Result Correctness and Reference Accuracy:** Utilized to evaluate the effectiveness of the memory module.\\n    *   **Efficiency:** An important aspect, especially for real-world applications, offering numeric strategies for comparison.\\n*   **Current State:** There is a lack of open-sourced benchmarks specifically tailored for memory modules in LLM-based agents.\\n\\n**2. Indirect Evaluation:**\\nThis strategy evaluates the memory module via end-to-end agent tasks, with the intuition that if an agent successfully completes a memory-dependent task, the designed memory module is effective.\\n*   **Methods:**\\n    *   **Ablation Studies:** Comparing the performance of agents with and without memory modules.\\n    *   **Task Completion:**\\n        *   **Conversation Tasks:** Such as long-context passage retrieval (finding correct paragraphs for given questions or descriptions) and long-context summarization (formulating a global understanding and summarizing context), utilizing metrics like ROUGE for summarization.\\n        *   **Utilizing Benchmarks and Environments:** Employing environments like ALFWorld, IGLU, Minecraft, Tachikuma (TRPG game logs), AgentBench (for autonomous agents across diverse environments), SocKET (for social capabilities), PEB (for penetration testing), ClemBench (Dialogue Games), and E2E (for chatbot accuracy and usefulness).\\n        *   **Multi-task Evaluation:** Using a set of diverse tasks from different domains to measure the agent's generalization capability.\\n        *   **Software Testing:** Evaluating agents by having them conduct tasks like generating test cases, reproducing bugs, or debugging code, using metrics such as test coverage and bug detection rate.\\n*   **Considerations:** While generally easier to conduct due to many public benchmarks, the performance on tasks can be attributed to various factors, not solely memory, which may lead to biased evaluation results.\",\n",
       " 'source_documents': [Document(metadata={'chunk_index': 107, 'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents_output.pdf'}, page_content='F1 = 2 · Precision · Recall\\n\\nwhere the precision and recall scores are calculated as Precision =\\nTP\\nTP+FP and Recall =\\nTP\\nTP+FN. The\\nTP represents the number of true positive memory contents, FP means the number of false positive\\nmemory contents, and FN indicates the number of false negative memory contents. In previous works,\\nLu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al. [6]\\nfocus on assessing whether related memory can be successfully retrieved.\\n\\nResult Correctness and Reference Accuracy are both utilized to evaluate the effectiveness of the\\nmemory module. Beyond effectiveness, efficiency is also an important aspect, especially for real-\\nworld applications. Therefore, we describe the evaluation of efficiency as follows.'),\n",
       "  Document(metadata={'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf', 'chunk_index': 109}, page_content='Objective evaluation offers numeric strategies to compare different methods of memory, which is\\nimportant to benchmark this field and promote future developments.\\n\\n6.2\\nIndirect Evaluation\\n\\nBesides the above method that directly evaluates the memory module, evaluating via task completion\\nis also a popular evaluation strategy. The intuition behind this type of approaches is that if the\\nagent can successfully complete a task that highly depends on memory, it suggests that the designed\\nmemory module is effective. In the following parts, we present several representative tasks that are\\nleveraged to evaluate the memory module in indirect ways.\\n\\n6.2.1\\nConversation'),\n",
       "  Document(metadata={'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf', 'chunk_index': 114}, page_content='In previous works, Huang et al. [19] organize a comprehensive survey for long-context LLMs, which\\nprovides a summary of evaluation metrics on long-context scenarios. Moreover, Shaham et al. [138]\\npropose a zero-shot benchmark for evaluating agents’ understanding of long-context natural languages.\\nAs for specific long-context tasks, long-context passage retrieval is one of the important tasks for\\nevaluating the long-context ability of agents. It requires agents to find the correct paragraph in a long\\ncontext that corresponds to the given questions or descriptions [139]. Long-context summarization\\nis another representative task. It requests agents to formulate a global understanding of the whole\\ncontext, and summarizes it according to the descriptions, where some metrics on matching scores\\nlike ROUGE can be utilized to compare the results with ground truths.'),\n",
       "  Document(metadata={'source': 'A Survey on Large Language Model based Autonomous Agents.pdf', 'chunk_index': 9}, page_content='acquisition. In addition to discussing agent con-\\nstruction, we also provide a systematic overview of\\nthe applications of LLM-based autonomous agents\\nin social science, natural science, and engineering.\\nFinally, we delve into the strategies for evaluating\\nLLM-based autonomous agents, focusing on both\\nsubjective and objective strategies.'),\n",
       "  Document(metadata={'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf', 'chunk_index': 11}, page_content='While previous studies have designed many promising memory modules, there still lacks a systemic\\nstudy to view the memory modules from a holistic perspective. To bridge this gap, in this paper,\\nwe comprehensively review previous studies to present clear taxonomies and key principles for\\ndesigning and evaluating the memory module. In specific, we discuss three key problems including:\\n(1) what is the memory of LLM-based agents? (2) why do we need the memory in LLM-based\\nagents? and (3) how to implement and evaluate the memory in LLM-based agents? To begin with,\\nwe detail the concepts of memory in LLM-based agents, providing both narrow and broad definitions.\\nThen, we analyze the necessity of memory in LLM-based agents, showing its importance from\\nthree perspectives including cognitive psychology, self-evolution, and agent applications. Based on\\nthe problems of “what” and “why”, we present commonly used strategies to design and evaluate'),\n",
       "  Document(metadata={'source': 'A Survey on Large Language Model based Autonomous Agents.pdf', 'chunk_index': 149}, page_content='Benchmarks: Given the metrics and protocols,\\na crucial aspect of evaluation is the selection of\\nappropriate benchmarks. Over time, various bench-\\nmarks have been introduced to assess the capa-\\nbilities of LLM-based agents across diverse do-\\nmains and scenarios. Many studies employ envi-\\nronments such as ALFWorld [60], IGLU [161],\\nand Minecraft [16, 33, 38] to evaluate agent ca-\\npabilities in interactive and task-oriented simula-\\ntions. Tachikuma [168] evaluates LLMs’ ability to\\ninfer and understand complex interactions involv-\\ning multiple characters and novel objects through\\nTRPG game logs. AgentBench [169] provides a\\ncomprehensive framework for evaluating LLMs as\\nautonomous agents across diverse environments. It\\nrepresents the first systematic assessment of LLMs\\nas agents on real-world challenges across diverse\\ndomains. SocKET [163] is a comprehensive bench-\\nmark for evaluating the social capabilities of LLMs\\nacross 58 tasks covering five categories of social'),\n",
       "  Document(metadata={'chunk_index': 98, 'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents_output.pdf'}, page_content='6\\nHow to Evaluate the Memory in LLM-based Agent\\n\\nHow to effectively evaluate the memory module remains an open problem, where diverse evaluation\\nstrategies have been proposed in previous works according to different applications. To clearly\\nshow the common ideas of different evaluation methods, in this section, we summarize a general\\nframework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\\nis, (1) direct evaluation, which independently measures the capability of the memory module. (2)\\nindirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\\nbe effectively accomplished, the memory module is demonstrated to be useful.\\n\\n6.1\\nDirect Evaluation'),\n",
       "  Document(metadata={'chunk_index': 102, 'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf'}, page_content='As for how to conduct the evaluation process, there are two important problems. The first one is how\\nto select the human evaluators. In general, the evaluators should be familiar with the evaluation task,\\nwhich ensures that the labeling results are convincing and reliable. In addition, the backgrounds of\\nthe evaluators should be diverse to remove subjective biases of specific human groups. The second\\nproblem is how to label the outputs of the memory module. Usually, one can either directly score the\\n\\nresults [6] or make comparisons between two candidates [95]. The former can obtain absolute and\\nquantitative evaluation results, while the latter can remove the labeling noises when independently\\nscoring each candidate. In addition, the granularity of the ratings should also be carefully designed.\\nToo coarse ratings may not effectively discriminate the capabilities of different memory modules,\\nwhile too fine-grained ones may bring more effort for the workers to make judgments.'),\n",
       "  Document(metadata={'source': 'The Rise and Potential of Large Language Model Based Agents.pdf', 'chunk_index': 72}, page_content='• Compressing memories with vectors or data structures. By employing suitable data structures,\\nintelligent agents boost memory retrieval efficiency, facilitating prompt responses to interactions.\\nNotably, several methodologies lean on embedding vectors for memory sections, plans, or dialogue\\nhistories [109; 170; 172; 174]. Another approach translates sentences into triplet configurations\\n[173], while some perceive memory as a unique data object, fostering varied interactions [176].\\nFurthermore, ChatDB [175] and DB-GPT [240] integrate the LLMrollers with SQL databases,\\nenabling data manipulation through SQL commands.'),\n",
       "  Document(metadata={'source': 'A Survey on Large Language Model based Autonomous Agents.pdf', 'chunk_index': 153}, page_content='tive emotions and measures the emotional states of\\nLLMs and human subjects using self-report scales.\\nPEB [125] is tailored for assessing LLM-based\\nagents in penetration testing scenarios, comprising\\n13 diverse targets from leading platforms. It of-\\nfers a structured evaluation across varying difficulty\\nlevels, reflecting real-world challenges for agents.\\nClemBench [165] contains five Dialogue Games to\\nassess LLMs’ ability as a player. E2E [174] serves\\nas an end-to-end benchmark for testing the accuracy\\nand usefulness of chatbots.'),\n",
       "  Document(metadata={'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf', 'chunk_index': 131}, page_content='uses an actor-critic reflection to improve the robustness of agents. Item agents and user agents are\\nequipped with different memories in [149], where item agents are endowed with dynamic memory\\nmodules designed to capture and preserve information pertinent to their intrinsic attributes and the\\ninclinations of their adopters. For user agents, the adaptive memory updating mechanism plays a\\npivotal role in aligning the agents’ operations with user behaviors and preferences. Wang et al. [102]\\nmemorize individualized user information like reviews or ratings for items, and acquire domain-\\nspecific knowledge and real-time information by web searching tools.'),\n",
       "  Document(metadata={'chunk_index': 117, 'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents_output.pdf'}, page_content='In fact, nearly all the memory-equipped agents can evaluate the effect of memory by ablation studies,\\ncomparing the performance between with/without memory modules. The evaluation on specific\\nscenarios can better reflect the significance of memory for the downstream applications practically.\\n\\n6.3\\nDiscussions\\n\\nCompared with direct evaluation, indirect evaluation via specific tasks can be easier to conduct, since\\nthere are already many public benchmarks. However, the performance on tasks can be attributed\\nto various factors, and memory is only one of them, which may make the evaluation results biased.\\nBy direct evaluation, the effectiveness of the memory module can be independently evaluated,\\nwhich improves the reliability of the evaluation results. However, to our knowledge, there are no\\nopen-sourced benchmarks tailored for the memory modules in LLM-based agents.\\n\\n7\\nMemory-enhanced Agent Applications'),\n",
       "  Document(metadata={'source': 'Augmented Language Models.pdf', 'chunk_index': 37}, page_content='Question: It takes Amy 4 minutes to climb to the top of a slide.\\nIt takes her 1 minute to\\nslide down. The water slide closes in 15 minutes. How many times can she slide before it closes?\\n<LM>\\n\\nAnswer:\\nTo solve “ How many times can she slide before it closes? ”, we need to first solve:\\n\\n“ How long does each trip take? ”\\n</LM>\\n\\nIt takes Amy 4 minutes to climb to the top of a slide.\\nIt takes her 1 minute to slide down.\\nThe water slide closes in 15 minutes.\\nSubquestion 1: How long does each trip take?\\n<LM>\\n\\nAnswer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip\\ntakes 5 minutes.\\n</LM>\\n\\nIt takes Amy 4 minutes to climb to the top of a slide.\\nIt takes her 1 minute to slide down.\\nThe slide closes in 15 minutes.\\nSubquestion 1: How long does each trip take?\\nAnswer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip\\ntakes 5 minutes.\\nSubquestion 2: How many times can she slide before it closes?\\n<LM>'),\n",
       "  Document(metadata={'source': 'A Survey on the Memory Mechanism of Large Language Model based Agents.pdf', 'chunk_index': 53}, page_content='In the above three perspectives, the first one reveals that the memory builds the cognitive basis of\\nthe agent. The second and third ones show that the memory is necessary for the agent’s evolving\\nprinciples and applications, which provide insights for designing agents with memory mechanisms.\\n\\n5\\nHow to Implement the Memory of LLM-based Agent\\n\\nIn this section, we discuss the implementation of the memory module from three perspectives: memory\\nsources, memory forms, and memory operations. Memory sources refer to where the memory contents\\ncome from. Memory forms focus on how to represent the memory contents. Memory operations\\naim to process the memory contents. These three perspectives provide a comprehensive review of\\nmemory implementation methods, which is helpful for future research. For better demonstration, we\\npresent an overview of implementation methods in Figure 4.\\n\\n5.1\\nMemory Sources'),\n",
       "  Document(metadata={'source': 'A Survey on Large Language Model based Autonomous Agents.pdf', 'chunk_index': 148}, page_content='(3) Multi-task evaluation: In this method, people\\nuse a set of diverse tasks from different domains to\\nevaluate the agent, which can effectively measure\\nthe agent generalization capability in open-domain\\nenvironments [12,29,86,151,162–164,169,170]. (4)\\nSoftware testing: In this method, researchers evalu-\\nate the agents by letting them conduct tasks such as\\nsoftware testing tasks, such as generating test cases,\\nreproducing bugs, debugging code, and interacting\\nwith developers and external tools [159,160,167].\\nThen, one can use metrics like test coverage and\\nbug detection rate to measure the effectiveness of\\nLLM-based agents.\\n\\n30\\nFront. Comput. Sci., 2025, 0(0): 1–42'),\n",
       "  Document(metadata={'source': 'Augmented Language Models.pdf', 'chunk_index': 20}, page_content='In recent years, prompting LMs to solve various downstream tasks has become a dominant paradigm (Brown\\net al., 2020). In prompting, examples from a downstream task are transformed such that they are formulated\\nas a language modeling problem. Prompting typically takes one of the two forms: zero-shot, where the\\nmodel is directly prompted with a test example’s input; and few-shot, where few examples of a task are\\nprepended along with a test example’s input. This few-shot prompting is also known as in-context learning\\nor few-shot learning. As opposed to “naive” prompting that requires an input to be directly followed by\\nthe output/answer, elicitive prompts encourage LMs to solve tasks by following intermediate steps before\\npredicting the output/answer. While Nye et al. (2021) provides the first example of few-shot prompting LLMs\\nwith reasoning examples and Cobbe et al. (2021) generalizes the use of reasoning examples to non-algorithmic')]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to effectively evaluate the memory module?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097af202-e298-4783-994c-dcffc5e65afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574e6e9-b6ad-4efb-b91c-2d314fdcb302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2755c3-fa72-4f87-b1e3-a0617e432bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
